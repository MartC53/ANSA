{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "from torch.nn.functional import pad\n",
    "import shutil\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "input_dir = \"G:/Shared drives/Posner Group Current/Cole's Files/ANSA/RPA on glass slides/100_serial/processed\"\n",
    "output_dir = \"G:/Shared drives/Posner Group Current/Cole's Files/ANSA/RPA on glass slides/100_serial/torch_tensors\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to get all image paths and their labels\n",
    "def get_image_paths_and_labels(root_dir):\n",
    "    image_data = []\n",
    "    for label in os.listdir(root_dir):\n",
    "        label_path = os.path.join(root_dir, label)\n",
    "        if os.path.isdir(label_path):\n",
    "            for idx, image_name in enumerate(os.listdir(label_path)):\n",
    "                if image_name.lower().endswith('.tif'):\n",
    "                    image_path = os.path.join(label_path, image_name)\n",
    "                    image_data.append((image_path, f\"{label}_{idx+1}\"))\n",
    "    return image_data\n",
    "\n",
    "# Get all image paths and labels\n",
    "image_data = get_image_paths_and_labels(input_dir)\n",
    "\n",
    "# Find the largest image dimensions across all time series stacks\n",
    "max_frames, max_height, max_width = 0, 0, 0\n",
    "for img_path, _ in image_data:\n",
    "    image_stack = imread(img_path)\n",
    "    f, h, w = image_stack.shape\n",
    "    max_frames = max(max_frames, f)\n",
    "    max_height = max(max_height, h)\n",
    "    max_width = max(max_width, w)\n",
    "\n",
    "print(f\"Largest dimensions found: {max_frames}x{max_height}x{max_width}\")\n",
    "\n",
    "# Pad and save images as .pt files\n",
    "for img_path, label in image_data:\n",
    "    image_stack = imread(img_path)  # Read the full 3D stack\n",
    "    f, h, w = image_stack.shape\n",
    "\n",
    "    # Calculate padding needed\n",
    "    pad_frames = max_frames - f\n",
    "    pad_height = max_height - h\n",
    "    pad_width = max_width - w\n",
    "\n",
    "    # Apply padding\n",
    "    padded_image = np.pad(image_stack, \n",
    "                          ((0, pad_frames), (0, pad_height), (0, pad_width)), \n",
    "                          mode='constant', constant_values=0)\n",
    "\n",
    "    # Convert to tensor and save\n",
    "    tensor_image = torch.tensor(padded_image, dtype=torch.float32).unsqueeze(0)  # Add channel dimension 1 for grayscale\n",
    "    save_path = os.path.join(output_dir, f\"{label}.pt\")\n",
    "    torch.save(tensor_image, save_path)\n",
    "\n",
    "    print(f\"Saved: {save_path}\")\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repadding to make them square ([1,180,499,500] currently)\n",
    "input_folder = r'../Datasets/torch_tensors/'\n",
    "output_folder = input_folder  # Change if you want to save elsewhere\n",
    "\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Desired shape\n",
    "target_shape = (1, 180, 500, 500)\n",
    "\n",
    "# Process each tensor file\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith('.pt'):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        \n",
    "        tensor = torch.load(file_path)\n",
    "\n",
    "        _, _, current_height, current_width = tensor.shape\n",
    "        \n",
    "        # Calculate padding amounts (last two dimensions: width and height)\n",
    "        pad_width = target_shape[-1] - current_width  # Padding for width (500 - 499)\n",
    "        pad_height = target_shape[-2] - current_height  # Padding for height (should be 0)\n",
    "\n",
    "        # Apply padding (pad format: last dimension first, so width then height)\n",
    "        padded_tensor = torch.nn.functional.pad(tensor, (0, pad_width, 0, pad_height), \"constant\", 0)\n",
    "\n",
    "        output_path = os.path.join(output_folder, file_name)\n",
    "        torch.save(padded_tensor, output_path)\n",
    "\n",
    "        print(f\"Processed: {file_name} -> New shape: {padded_tensor.shape}\")\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = r'../Datasets/torch_tensors'\n",
    "output_dir = 'E:/Datasets/Split'\n",
    "\n",
    "random.seed(53)\n",
    "\n",
    "train_dir = os.path.join(output_dir, 'Training')\n",
    "test_dir = os.path.join(output_dir, 'Testing')\n",
    "val_dir = os.path.join(output_dir, 'Validation')\n",
    "\n",
    "os.makedirs(path, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "all_files = [f for f in os.listdir(source_dir) if f.endswith('.pt')]\n",
    "\n",
    "random.shuffle(all_files)\n",
    "print(\"Shuffle done\")\n",
    "num_files = len(all_files)\n",
    "train_split = int(num_files * 0.60)\n",
    "test_split = int(num_files * 0.20)\n",
    "\n",
    "train_files = all_files[:train_split]\n",
    "test_files = all_files[train_split:train_split + test_split]\n",
    "val_files = all_files[train_split + test_split:]  # Remainder goes here\n",
    "\n",
    "def copy_files(file_list, dest_dir):\n",
    "    for file in file_list:\n",
    "        shutil.copy(os.path.join(source_dir, file), os.path.join(dest_dir, file))\n",
    "\n",
    "copy_files(train_files, train_dir)\n",
    "print(\"Train copy done\")\n",
    "\n",
    "copy_files(test_files, test_dir)\n",
    "print(\"Test copy done\")\n",
    "\n",
    "copy_files(val_files, val_dir)\n",
    "print(\"Validation copy done\")\n",
    "\n",
    "\n",
    "print(f\"Copied {len(train_files)} files to Training\")\n",
    "print(f\"Copied {len(test_files)} files to Testing\")\n",
    "print(f\"Copied {len(val_files)} files to Validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training directory and output category folders\n",
    "Paths = ['E:/Datasets/Split/Testing/', 'E:/Datasets/Split/Training/',\n",
    "  'E:/Datasets/Split/Validation/']\n",
    "\n",
    "For Path in Paths:\n",
    "    train_dir = Path\n",
    "    category_dirs = {\n",
    "        'undetectable': os.path.join(train_dir, 'undetectable'),\n",
    "        'low': os.path.join(train_dir, 'low'),\n",
    "        'medium': os.path.join(train_dir, 'medium'),\n",
    "        'high': os.path.join(train_dir, 'high')\n",
    "    }\n",
    "\n",
    "    # Create category directories if they don't exist\n",
    "    for dir_path in category_dirs.values():\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    # Function to categorize based on label value\n",
    "    def categorize_file(filename):\n",
    "        try:\n",
    "            label = int(filename.split('_')[0])  # Extract numeric label before underscore\n",
    "            if label < 200:\n",
    "                return 'undetectable'\n",
    "            elif 200 <= label <= 1000:\n",
    "                return 'low'\n",
    "            elif 1000 < label <= 10000:\n",
    "                return 'medium'\n",
    "            else:\n",
    "                return 'high'\n",
    "        except ValueError:\n",
    "            print(f\"Skipping {filename}: Invalid format\")\n",
    "            return None\n",
    "\n",
    "    # Process each file in the training directory\n",
    "    for file in os.listdir(train_dir):\n",
    "        if file.endswith('.pt'):\n",
    "            category = categorize_file(file)\n",
    "            if category:\n",
    "                src_path = os.path.join(train_dir, file)\n",
    "                dest_path = os.path.join(category_dirs[category], file)\n",
    "                shutil.copy(src_path, dest_path)\n",
    "\n",
    "    print(\"Files have been copied to respective categories:\")\n",
    "    for category, path in category_dirs.items():\n",
    "        print(f\"{category}: {len(os.listdir(path))} files\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ANSA_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
